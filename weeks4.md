---
layout: post
title: Week 4
by: Victoria Spriggs
---
This week, I completed the updated RAG model using langchain and transformers. Retrieval Augmented Generation (RAG) is a technique used to decrease hallucinations and make the model more accurate. To use it, you need to create an additional model that can take whatever documents you want to feed to the LLM for it to use for a response. This week, I also experimented with a few different prompt formats that could be used for the model. After researching, I found some prompting techniques that I think will be useful in getting a good response from the LLM. I was able to get output from the RAG model, however, it wasn’t the desired output. I ran into some issues later in the code that prevented it from giving the actual production that we wanted. The specific error message I got was that the argument/ parameters needed to be of type (SquadExample, dict). I’ve spent time trying to fix this issue, but it was better for me to work with the rest of my team to fix it. Another challenge I faced was issues with the memory available on my computer. Halfway through the program, it would always crash because at that point I had used up all of the RAM available. That was a simple fix because all I had to do was get more RAM. Since we are using Google Colab, that was as simple as signing up for a subscription. However, another issue that I wasn’t paying attention to, the speed of the program, was fixed after purchasing more memory. With adding all of these extra elements, it made the program slow again. That made the program much smoother and easier to run. 
